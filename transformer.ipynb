{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model  \n",
        "Predicting Droughts Using Weather & Soil Data in the US\n",
        "\n",
        "## Instructions  \n",
        "This notebook is designed to run on Google Colab. It requires **at least 25 GB of system RAM** and **1.5 GB of GPU RAM**, so a **T4 GPU with high-RAM runtime** is recommended.\n",
        "\n",
        "You do not need to manually install any datasets. Simply add your Kaggle credentials in the designated code cell, and the data will be downloaded and saved to your Google Drive.\n",
        "\n",
        "If you plan to run this notebook on another machine (e.g., your local computer), you can skip the `drive.mount` step and change the data directory path to your local destination. Everything else should work as expected.\n"
      ],
      "metadata": {
        "id": "Z3MEmtvlyeW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dataset"
      ],
      "metadata": {
        "id": "HBoxbCyLyriW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFRPOecd9rze",
        "outputId": "e297ae8b-980a-437d-a40a-6bb6efd1996b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# change your kaggle information here\n",
        "os.environ['KAGGLE_USERNAME'] = \"YourKaggleUsername\"\n",
        "os.environ['KAGGLE_KEY'] = \"YourKaggleKey\"\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/us_drought_data\"  # folder in Google Drive\n",
        "ZIP_PATH = os.path.join(DATA_DIR, \"us-drought-meteorological-data.zip\")\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "soil_csv  = os.path.join(DATA_DIR, \"soil_data.csv\")\n",
        "train_csv = os.path.join(DATA_DIR, \"train_timeseries/train_timeseries.csv\")\n",
        "val_csv   = os.path.join(DATA_DIR, \"validation_timeseries/validation_timeseries.csv\")\n",
        "test_csv  = os.path.join(DATA_DIR, \"test_timeseries/test_timeseries.csv\")\n",
        "\n",
        "\n",
        "if not (os.path.isfile(soil_csv) and\n",
        "        os.path.isfile(train_csv) and\n",
        "        os.path.isfile(val_csv) and\n",
        "        os.path.isfile(test_csv)):\n",
        "    !kaggle datasets download -d cdminix/us-drought-meteorological-data -p {DATA_DIR} -o\n",
        "    !unzip -q {ZIP_PATH} -d {DATA_DIR}\n",
        "    print(\"\\nDataset downloaded and unzipped to Google Drive.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nDataset already found in Google Drive. Skipping download.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63G7uTnX_aqx",
        "outputId": "2dab5c8a-cd2b-43cc-9967-7d1b0e61753e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset already found in Google Drive. Skipping download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "c6LjS3b8yvy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "an3wsiTeyz1b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "g8BmTa4ey24j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamic feature selection: drops any feature if it is no longer useful as an indicator\n",
        "weather_features = [\n",
        "    \"PRECTOT\",\"PS\",\"QV2M\",\"T2M\",\"T2MDEW\",\"T2MWET\",\"T2M_MAX\",\"T2M_MIN\",\n",
        "    \"T2M_RANGE\",\"TS\",\"WS10M\",\"WS10M_MAX\",\"WS10M_MIN\",\"WS10M_RANGE\",\n",
        "    \"WS50M\",\"WS50M_MAX\",\"WS50M_MIN\",\"WS50M_RANGE\"\n",
        "]\n",
        "soil_features = [\n",
        "    \"lat\",\"lon\",\"elevation\",\"slope1\",\"slope2\",\"slope3\",\"slope4\",\"slope5\",\n",
        "    \"slope6\",\"slope7\",\"slope8\",\"aspectN\",\"aspectE\",\"aspectS\",\"aspectW\",\n",
        "    \"aspectUnknown\",\"WAT_LAND\",\"NVG_LAND\",\"URB_LAND\",\"GRS_LAND\",\"FOR_LAND\",\n",
        "    \"CULTRF_LAND\",\"CULTIR_LAND\",\"CULT_LAND\",\"SQ1\",\"SQ2\",\"SQ3\",\"SQ4\",\"SQ5\",\n",
        "    \"SQ6\",\"SQ7\"\n",
        "]\n",
        "\n",
        "# fill nan values with smoothly interpolated values\n",
        "# e.g [0, nan, nan, nan, 1] -> [0, 0.25, 0.5, 0.75, 1]\n",
        "def interpolate_group(df, cols):\n",
        "    arr = df[cols].values\n",
        "    idx = np.arange(len(df))\n",
        "    for j in range(arr.shape[1]):\n",
        "        col = arr[:, j]\n",
        "        mask = ~np.isnan(col)\n",
        "        if mask.any():\n",
        "            arr[:, j] = np.interp(idx, idx[mask], col[mask])\n",
        "    df[cols] = arr\n",
        "    return df\n",
        "\n",
        "# compute the median and iqr on train dataset\n",
        "def robust_fit(values: np.ndarray):\n",
        "    median = np.nanmedian(values)\n",
        "    q75 = np.nanpercentile(values, 75)\n",
        "    q25 = np.nanpercentile(values, 25)\n",
        "    iqr = max(q75 - q25, 1e-6)\n",
        "    return median, iqr\n",
        "\n",
        "# apply to train, test, and val datasets to reduce the impact of outliers\n",
        "def robust_transform(values: np.ndarray, median: float, iqr: float):\n",
        "    return (values - median) / iqr\n",
        "\n",
        "def load_datasets(data_dir):\n",
        "    soil  = pd.read_csv(os.path.join(data_dir, \"soil_data.csv\"))\n",
        "    train = pd.read_csv(os.path.join(data_dir, \"train_timeseries/train_timeseries.csv\"))\n",
        "    val   = pd.read_csv(os.path.join(data_dir, \"validation_timeseries/validation_timeseries.csv\"))\n",
        "    test  = pd.read_csv(os.path.join(data_dir, \"test_timeseries/test_timeseries.csv\"))\n",
        "\n",
        "    for df in (train, val, test):\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "        df.sort_values([\"fips\",\"date\"], inplace=True)\n",
        "        doy = df[\"date\"].dt.dayofyear\n",
        "        df[\"season_sin\"] = np.sin(2*np.pi*(doy-1)/365)\n",
        "        df[\"season_cos\"] = np.cos(2*np.pi*(doy-1)/365)\n",
        "        df[\"score_past\"] = df.groupby(\"fips\")[\"score\"].shift(7)\n",
        "\n",
        "    # merge soil data (static) with time series data\n",
        "    train = train.merge(soil, on=\"fips\", how=\"left\")\n",
        "    val   = val.merge(soil,   on=\"fips\", how=\"left\")\n",
        "    test  = test.merge(soil,  on=\"fips\", how=\"left\")\n",
        "\n",
        "    # interpolate weather + score_past\n",
        "    interp_cols = weather_features + [\"score_past\"]\n",
        "    train = train.groupby(\"fips\", group_keys=False).apply(interpolate_group, interp_cols)\n",
        "    val   = val.  groupby(\"fips\", group_keys=False).apply(interpolate_group, interp_cols)\n",
        "    test  = test. groupby(\"fips\", group_keys=False).apply(interpolate_group, interp_cols)\n",
        "\n",
        "    # fill static nan with train mean\n",
        "    for feat in soil_features:\n",
        "        mean_val = train[feat].mean()\n",
        "        for df in (train, val, test):\n",
        "            df[feat].fillna(mean_val, inplace=True)\n",
        "\n",
        "    # robust scaling & some feature engineering\n",
        "    dyn_feats = weather_features + [\"season_sin\",\"season_cos\",\"score_past\"]\n",
        "    stat_feats = soil_features\n",
        "\n",
        "    # fit scale params on train only\n",
        "    params_dyn  = {c: robust_fit(train[c].values) for c in dyn_feats}\n",
        "    params_stat = {c: robust_fit(train[c].values) for c in stat_feats}\n",
        "\n",
        "    for df in (train, val, test):\n",
        "        for c in dyn_feats:\n",
        "            med, iqr = params_dyn[c]\n",
        "            df[c] = robust_transform(df[c].values, med, iqr)\n",
        "        for c in stat_feats:\n",
        "            med, iqr = params_stat[c]\n",
        "            df[c] = robust_transform(df[c].values, med, iqr)\n",
        "\n",
        "    return train, val, test, dyn_feats, stat_feats"
      ],
      "metadata": {
        "id": "enE3oby5y6oH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DroughtDataset(Dataset):\n",
        "    def __init__(self, df, dyn_feats, sta_feats, seq_len=240, horizon=6):\n",
        "        self.seq_len, self.horizon = seq_len, horizon\n",
        "        self.dyn, self.sta = dyn_feats, sta_feats\n",
        "        self.data, self.indices = {}, []\n",
        "\n",
        "        df = df.sort_values([\"fips\",\"date\"])\n",
        "        for fips, g in df.groupby(\"fips\"):\n",
        "            Xd = g[self.dyn].values.astype(np.float32)\n",
        "            Xs = g[self.sta].iloc[0].values.astype(np.float32)\n",
        "            y  = g[\"score\"].values\n",
        "            self.data[fips] = {\"Xd\":Xd, \"Xs\":Xs, \"y\":y}\n",
        "\n",
        "            max_start = len(g) - seq_len - 7*horizon + 1\n",
        "            for i in range(max_start):\n",
        "                idxs = [i+seq_len+7*w for w in range(horizon)]\n",
        "                if not np.isnan(y[idxs]).any():\n",
        "                    self.indices.append((fips, i))\n",
        "\n",
        "    def __len__(self): return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fips, start = self.indices[idx]\n",
        "        d = self.data[fips]\n",
        "        X_dyn = torch.from_numpy(d[\"Xd\"][start:start+self.seq_len])\n",
        "        X_sta = torch.from_numpy(d[\"Xs\"])\n",
        "        y_idx = [start+self.seq_len+7*w for w in range(self.horizon)]\n",
        "        y = torch.from_numpy(d[\"y\"][y_idx].astype(np.float32))\n",
        "        return X_dyn, X_sta, y"
      ],
      "metadata": {
        "id": "a6n5EqbY0vF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "4FWRV6D-zFrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyper parameter and constant here\n",
        "# play with different hyper parameter to get the best result\n",
        "EPOCHS       = 10\n",
        "BATCH        = 2048\n",
        "SEQ_LEN      = 7\n",
        "HORIZON      = 6\n",
        "HIDDEN       = 256\n",
        "LAYERS       = 3\n",
        "HEADS        = 8\n",
        "DROPOUT      = 0.5\n",
        "LR           = 3e-4\n",
        "GRAD_CLIP    = 1.0\n",
        "USE_SWA      = True\n",
        "SWA_START_EP = 4\n",
        "CKPT_PATH    = \"best_transformer_mae.pt\"\n",
        "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rgfGOsIQl2cz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DroughtTransformer(nn.Module):\n",
        "    def __init__(self, dyn_in, sta_in, hidden, layers, heads,\n",
        "                 seq_len, horizon, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(dyn_in, hidden) # dynamic, time series data\n",
        "        self.register_buffer(\"pos\", self._positional_encoding(seq_len, hidden))\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            hidden, heads, dim_feedforward=hidden*4,\n",
        "            dropout=dropout, batch_first=True)\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, layers)\n",
        "        self.fc_sta = nn.Sequential(\n",
        "            nn.LayerNorm(sta_in),\n",
        "            nn.Linear(sta_in, hidden),\n",
        "            nn.ReLU()\n",
        "        ) # soil data\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(hidden*2, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, horizon)\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _positional_encoding(seq_len, d_model):\n",
        "        pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                        -(math.log(10000.0) / d_model))\n",
        "        pe  = torch.zeros(seq_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        return pe.unsqueeze(0)        # (1, seq_len, d_model)\n",
        "\n",
        "    def forward(self, x_dyn, x_sta):\n",
        "        x_dyn = self.embed(x_dyn) + self.pos[:, :x_dyn.size(1)]\n",
        "        h_dyn = self.enc(x_dyn)[:, -1]\n",
        "        h_sta = self.fc_sta(x_sta)\n",
        "        h = torch.cat([h_dyn, h_sta], 1)\n",
        "        return self.fc_out(h)"
      ],
      "metadata": {
        "id": "s0I3OMH0z5w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_ld, val_ld):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    sched = OneCycleLR(opt, max_lr=LR*10, epochs=EPOCHS,\n",
        "                       steps_per_epoch=len(train_ld),\n",
        "                       pct_start=0.3, anneal_strategy=\"cos\",\n",
        "                       div_factor=10, final_div_factor=1e4)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    best_mae = float(\"inf\"); bad=0; patience=3\n",
        "    swa_model = AveragedModel(model) if USE_SWA else None\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_ld, desc=f\"[{ep}/{EPOCHS}]\")\n",
        "        for Xd, Xs, y in pbar:\n",
        "            Xd, Xs, y = Xd.to(DEVICE), Xs.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with autocast():\n",
        "                pred = model(Xd, Xs)\n",
        "                loss = F.l1_loss(pred, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(opt)\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(opt); scaler.update(); sched.step()\n",
        "            pbar.set_postfix(mae=f\"{loss.item():.4f}\")\n",
        "\n",
        "        # validation\n",
        "        model.eval();\n",
        "        mae_sum = 0.0;\n",
        "        n = 0\n",
        "        with torch.no_grad(), autocast():\n",
        "            for Xd, Xs, y in val_ld:\n",
        "                Xd, Xs, y = Xd.to(DEVICE), Xs.to(DEVICE), y.to(DEVICE)\n",
        "                pred = model(Xd, Xs)\n",
        "                mae_sum += F.l1_loss(pred, y, reduction='sum').item()\n",
        "                n += y.numel()\n",
        "        val_mae = mae_sum / n\n",
        "        print(f\"  ↳ val MAE={val_mae:.4f}\")\n",
        "\n",
        "        if val_mae < best_mae:\n",
        "            best_mae = val_mae; bad=0\n",
        "            torch.save({\"model\":model.state_dict()}, CKPT_PATH)\n",
        "            print(\"    ✓ best saved\")\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                print(\"    ✕ early stop\"); break\n",
        "\n",
        "        if USE_SWA and ep >= SWA_START_EP:\n",
        "            swa_model.update_parameters(model)\n",
        "\n",
        "    # finalize SWA\n",
        "    if USE_SWA and swa_model is not None and ep >= SWA_START_EP:\n",
        "        update_bn(train_ld, swa_model, device=DEVICE)\n",
        "        torch.save({\"model\":swa_model.state_dict()}, \"best_swa.pt\")"
      ],
      "metadata": {
        "id": "NpyF6EHsz877"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and run"
      ],
      "metadata": {
        "id": "B4iXlKLBzQBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dl):\n",
        "    model.eval();\n",
        "    mae_sum = 0.0;\n",
        "    n = 0\n",
        "    with torch.no_grad(), autocast():\n",
        "        for Xd, Xs, y in tqdm(dl, desc=\"[test]\"):\n",
        "            Xd, Xs, y = Xd.to(DEVICE), Xs.to(DEVICE), y.to(DEVICE)\n",
        "            pred = model(Xd, Xs)\n",
        "            mae_sum += F.l1_loss(pred, y, reduction='sum').item()\n",
        "            n += y.numel()\n",
        "    return mae_sum / n"
      ],
      "metadata": {
        "id": "UmDLYCmRzTCm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading & preprocessing …\")\n",
        "train_df, val_df, test_df, dyn_feats, sta_feats = load_datasets(DATA_DIR)\n",
        "\n",
        "train_ds = DroughtDataset(train_df, dyn_feats, sta_feats, SEQ_LEN, HORIZON)\n",
        "val_ds   = DroughtDataset(val_df,   dyn_feats, sta_feats, SEQ_LEN, HORIZON)\n",
        "test_ds  = DroughtDataset(test_df,  dyn_feats, sta_feats, SEQ_LEN, HORIZON)\n",
        "\n",
        "train_ld = DataLoader(train_ds, BATCH, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_ld   = DataLoader(val_ds,   BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_ld  = DataLoader(test_ds,  BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "model = DroughtTransformer(\n",
        "    dyn_in=len(dyn_feats), sta_in=len(sta_feats),\n",
        "    hidden=HIDDEN, layers=LAYERS, heads=HEADS,\n",
        "    seq_len=SEQ_LEN, horizon=HORIZON, dropout=DROPOUT\n",
        ").to(DEVICE)\n",
        "\n",
        "print(\"Training …\")\n",
        "train(model, train_ld, val_ld)\n",
        "\n",
        "print(\"\\nEvaluating best checkpoint …\")\n",
        "ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model\"])\n",
        "test_mae = evaluate(model, test_ld)\n",
        "print(f\"Test MAE = {test_mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNw-1FfpzWZq",
        "outputId": "4a4c50a7-e322-427b-ecb7-de09979d87f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading & preprocessing …\n",
            "Training …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1/10]: 100%|██████████| 1336/1336 [01:04<00:00, 20.62it/s, mae=0.2725]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ val MAE=0.2693\n",
            "    ✓ best saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2/10]: 100%|██████████| 1336/1336 [01:05<00:00, 20.32it/s, mae=0.3195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ val MAE=0.2492\n",
            "    ✓ best saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[3/10]: 100%|██████████| 1336/1336 [01:05<00:00, 20.37it/s, mae=0.3198]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ val MAE=0.3340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[4/10]: 100%|██████████| 1336/1336 [01:05<00:00, 20.33it/s, mae=0.3168]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ val MAE=0.2883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[5/10]: 100%|██████████| 1336/1336 [01:05<00:00, 20.32it/s, mae=0.2998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ val MAE=0.2773\n",
            "    ✕ early stop\n",
            "\n",
            "Evaluating best checkpoint …\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[test]: 100%|██████████| 149/149 [00:03<00:00, 45.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE = 0.1982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}